{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Gradient Descent\n",
    "\n",
    "Goal: Implement a simple logistic regression model using gradient descent and forward/backward propogation ideas from neural networks. The basic idea follows from the Deep Learning course by deeplearning.ai, but the propogation functions have been reorganized by myself to match concepts from vector calculus more closely. In particular, the total derivative as a matrix and the chain rule as matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the model, I want to answer the following\n",
    "\n",
    "**Question**: Is it faster to use a sum or a dot product to find the sum of the elements in a vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505.70772442755737\n",
      "Dot product sum: 0.0820159912109375 ms\n",
      "505.70772442755737\n",
      "Numpy sum: 0.09202957153320312 ms\n"
     ]
    }
   ],
   "source": [
    "v = np.random.rand(1000,1)\n",
    "u = np.full((1,1000), 1)\n",
    "\n",
    "tic = time.time()\n",
    "total = np.dot(u, v)\n",
    "toc = time.time()\n",
    "print(total[0,0])\n",
    "print('Dot product sum: {} ms'.format((toc-tic)*1000))\n",
    "\n",
    "tic = time.time()\n",
    "total = np.sum(v, axis=0)\n",
    "toc = time.time()\n",
    "print(total[0])\n",
    "print('Numpy sum: {} ms'.format((toc-tic)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell several times, it appears that the dot product is usually faster. Sometimes the np.sum method is faster, so let's find an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product elapsed time: 29.576539993286133 ms\n",
      "Numpy sum elapsed time: 58.959245681762695 ms\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "\n",
    "dot_total = 0\n",
    "sum_total = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    v = np.random.rand(1000,1)\n",
    "    u = np.full((1,1000), 1)\n",
    "    \n",
    "    tic = time.time()\n",
    "    total = np.dot(u, v)\n",
    "    toc = time.time()\n",
    "    dot_total += toc-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    total = np.sum(v, axis=0)\n",
    "    toc = time.time()\n",
    "    sum_total += toc-tic\n",
    "    \n",
    "print('Dot product elapsed time: {} ms'.format(1000*dot_total))\n",
    "print('Numpy sum elapsed time: {} ms'.format(1000*sum_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above test, it appears that the dot product is, on average, about twice as fast as the numpy sum.\n",
    "\n",
    "I will use a dot product in my backwards propogation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Sigmoid/logistic function.\n",
    "    \n",
    "        x = numpy array or float'''\n",
    "    \n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y, yhat):\n",
    "    '''Loss function for logistic regression.\n",
    "    \n",
    "        y = true (binary) class\n",
    "        yhat = predicted class probability'''\n",
    "    \n",
    "    return -(y*np.log(yhat)+(1-y)*np.log(1-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(X, Y, w, b):\n",
    "    '''Implement forward and backward propogation for simple logistic regression.\n",
    "    \n",
    "        X = n_x by m numpy array containg training data where n_x is the number of features and m is the number of samples\n",
    "        Y = m by 1 numpy array containing true sample classes\n",
    "        w = m by 1 numpy array of model parameters\n",
    "        b = model bias (float)'''\n",
    "    \n",
    "    m = X.shape[1]    # number of training samples\n",
    "    n_x = X.shape[0]    # number of features\n",
    "    \n",
    "    w.reshape(n_x,1)    # force w to be a column vector\n",
    "    \n",
    "    # Forward propogation\n",
    "    A = sigmoid(np.dot(X.T,w)+b)\n",
    "    \n",
    "    # Backward propogation\n",
    "    dw = (1/m)*np.dot(np.full((1,m),1), (A-Y)*X.T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    # Cost for current parameter values\n",
    "    cost = (1/m)*np.sum(logloss(Y, A))\n",
    "    \n",
    "    gradients = dict({'dw': dw, 'db': db})\n",
    "    \n",
    "    return gradients, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a fitting function to loop through forward and backward propogation to fit model parameters via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, w, b, iterations=2000, learning_rate=0.01):\n",
    "    '''Implement gradient descent fitting procedure.\n",
    "    \n",
    "        X = n_x by m array with samples in columns\n",
    "        Y = 1 by m array of true binary class values\n",
    "        w = n_x by 1 array of model parameters\n",
    "        b = model bias'''\n",
    "    \n",
    "    n_x = X.shape[0]    # number of features\n",
    "    m = X.shape[1]    # number of training samples\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        gradient, cost = propogate(X, Y, w, b)\n",
    "        dw = gradient['dw']\n",
    "        db = gradient['db']\n",
    "        w = w - learning_rate*dw.reshape(n_x,1)\n",
    "        b = b - learning_rate*db\n",
    "    \n",
    "    parameters = dict({'w': w, 'b': b})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to initialize model parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X):\n",
    "    '''Initialize logistic regression parameters to zero.\n",
    "    \n",
    "        X = n_x by m array where n_x is the number of model features\n",
    "        \n",
    "    Returns a numpy array w of zeros in the shape n_x by 1 and a scaler b=0.'''\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    w = np.zeros((n_x,1))\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict binary class values using logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_val(x):\n",
    "    '''Find the most likely class given the input probability x.'''\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# use numpy vectorization to apply class_val to an array\n",
    "vclass_val = np.vectorize(class_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    '''Predict class using logistic regression model.\n",
    "    \n",
    "        X = array with n_x rows, samples in columns'''\n",
    "    \n",
    "    A = sigmoid(np.dot(X.T, w)+b)\n",
    "    \n",
    "    return vclass_val(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge functions into one that both trains and tests a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogModel(X_train, Y_train, X_test, Y_test, iterations=2000, learning_rate=0.01):\n",
    "    '''Build logistic regression model using training data and test on provided\n",
    "    testing data.\n",
    "    \n",
    "        X_train = n_x by m array with feature vectors in columns\n",
    "        Y_train = m by 1 array of true binary class for each training sample\n",
    "        X_test = n_x by any number array with feature vectors in columns\n",
    "        Y_test = column vector of true binary class for each test sample\n",
    "        iterations = positive integer, number of iterations for gradient descent\n",
    "        learning_rate = positive float, learning rate used for gradient descent\n",
    "        \n",
    "    Prints percentage of correct predictions on testing data using the model fit by gradient descent.\n",
    "    Returns model parameters.'''\n",
    "    \n",
    "    w, b = initialize(X_train)\n",
    "    \n",
    "    parameters = fit(X_train, Y_train, w, b, iterations=iterations, learning_rate=learning_rate)\n",
    "    w = parameters['w']\n",
    "    b = parameters['b']\n",
    "    \n",
    "    predictions = predict(X_test, w, b)\n",
    "    \n",
    "    model_accuracy = 100-np.average(np.abs(Y_test-predictions))*100\n",
    "    \n",
    "    print('Model accuracy: {:.4f}%'.format(model_accuracy))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test to see if the function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': array([[-1.30787641],\n",
       "        [-0.08969751]]),\n",
       " 'b': 1.9820561991873469}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1],[0],[1]]), np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1],[0],[1]])\n",
    "\n",
    "LogModel(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import a larger data set from an insurance company project to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '/Users/connorodell/Documents/Data_Science/learning/exercise_03_train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), train_df['y'], test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 88.6622%\n"
     ]
    }
   ],
   "source": [
    "# run new model\n",
    "# note that samples are in rows of X_train and X_test, so the transposes are fed into the models\n",
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[-1.14780844e-01],\n",
       "        [ 2.70488916e-01],\n",
       "        [ 1.89860217e-01],\n",
       "        [-2.26686510e-01],\n",
       "        [-8.95049221e-02],\n",
       "        [-3.01273565e-02],\n",
       "        [-9.47080194e-04],\n",
       "        [ 7.72610182e-05],\n",
       "        [-2.65253764e-02],\n",
       "        [ 1.72796595e-02],\n",
       "        [-1.91301767e-01],\n",
       "        [-9.16689184e-03],\n",
       "        [-1.90020093e-01],\n",
       "        [ 2.88417777e-03],\n",
       "        [-2.30216866e-02],\n",
       "        [-9.34131494e-03],\n",
       "        [-1.42098072e-02],\n",
       "        [-8.99694480e-03],\n",
       "        [-1.56222311e-02],\n",
       "        [ 1.68186654e-01],\n",
       "        [-1.39099556e-01],\n",
       "        [-1.68681706e-01],\n",
       "        [-1.51535789e-01],\n",
       "        [ 1.20667576e-02],\n",
       "        [ 7.28119833e-03],\n",
       "        [-1.14011350e-01],\n",
       "        [ 1.86661284e-02],\n",
       "        [ 5.09663881e-02],\n",
       "        [ 4.26414065e-03],\n",
       "        [-1.23649297e-02],\n",
       "        [-3.51880100e-02],\n",
       "        [-1.67117936e-02],\n",
       "        [ 3.07172188e-03],\n",
       "        [ 1.71751525e-01],\n",
       "        [ 7.43580593e-03],\n",
       "        [-4.81171672e-01],\n",
       "        [ 2.86701810e-02],\n",
       "        [-7.77447675e-03],\n",
       "        [ 1.03015253e-01],\n",
       "        [-3.02558645e-01],\n",
       "        [-2.91754301e-02],\n",
       "        [-9.67373569e-02],\n",
       "        [ 1.19579135e-01],\n",
       "        [-8.85553458e-02],\n",
       "        [ 1.52658815e-03],\n",
       "        [-1.83768475e-01],\n",
       "        [-1.25832832e-02],\n",
       "        [-2.34408132e-02],\n",
       "        [-2.55645827e-01],\n",
       "        [ 2.70720861e-01],\n",
       "        [ 1.69103738e-02],\n",
       "        [-2.27026522e-04],\n",
       "        [ 5.26546117e-03],\n",
       "        [ 4.01520952e-03],\n",
       "        [-2.94108969e-01],\n",
       "        [ 3.44901626e-02],\n",
       "        [ 4.22605607e-01],\n",
       "        [-1.17859719e-02],\n",
       "        [ 7.08781733e-03],\n",
       "        [ 2.00785230e-02],\n",
       "        [ 2.05261042e-03],\n",
       "        [ 3.14960317e-01],\n",
       "        [-9.66859341e-03],\n",
       "        [ 9.02930270e-03],\n",
       "        [-2.33538153e-01],\n",
       "        [-7.95281598e-03],\n",
       "        [ 1.23824045e-01],\n",
       "        [-1.81362314e-01],\n",
       "        [-8.50150156e-02],\n",
       "        [ 3.02287370e-02],\n",
       "        [ 2.14731372e-01],\n",
       "        [ 4.26237061e-02],\n",
       "        [-5.10972918e-01],\n",
       "        [-5.77374944e-03],\n",
       "        [ 5.74255791e-02],\n",
       "        [ 2.17455731e-01],\n",
       "        [ 1.67254023e-01],\n",
       "        [ 1.17751773e-01],\n",
       "        [ 1.38845120e-02],\n",
       "        [ 5.67729558e-03],\n",
       "        [ 2.33956548e-01],\n",
       "        [ 3.41848160e-04],\n",
       "        [ 1.74527111e-01],\n",
       "        [ 2.97811322e-04],\n",
       "        [ 5.17185676e-03],\n",
       "        [ 6.88199975e-03],\n",
       "        [ 7.60803292e-03],\n",
       "        [ 4.52032577e-02],\n",
       "        [-9.77494092e-04],\n",
       "        [-1.71138754e-02],\n",
       "        [ 2.50264294e-02],\n",
       "        [-8.95729025e-02],\n",
       "        [-2.20497056e-01],\n",
       "        [ 5.01608640e-01],\n",
       "        [ 2.92283326e-02],\n",
       "        [ 2.36115636e-01],\n",
       "        [ 1.17120376e-02],\n",
       "        [ 2.02729540e-03],\n",
       "        [-1.31776624e-03],\n",
       "        [-1.40526530e-02],\n",
       "        [ 4.54788291e-03],\n",
       "        [ 2.97910770e-03],\n",
       "        [-1.69943671e-03],\n",
       "        [ 1.66049700e-02],\n",
       "        [-2.29322136e-03],\n",
       "        [-1.29951936e-02],\n",
       "        [-4.00438351e-02],\n",
       "        [ 3.12261756e-02],\n",
       "        [-8.52425236e-02],\n",
       "        [ 8.42385760e-02],\n",
       "        [ 6.71777249e-02],\n",
       "        [-1.24246996e-02],\n",
       "        [ 1.91947986e-02],\n",
       "        [ 3.79002099e-02],\n",
       "        [ 5.48242752e-03],\n",
       "        [-1.13203413e-02],\n",
       "        [-1.74716237e-02],\n",
       "        [-4.45096796e-02],\n",
       "        [ 6.39028542e-03],\n",
       "        [-2.03592487e-02],\n",
       "        [ 5.19306559e-02],\n",
       "        [ 7.14637088e-02],\n",
       "        [ 4.49465150e-02],\n",
       "        [ 6.61497895e-04],\n",
       "        [ 4.19481655e-03],\n",
       "        [-8.14712080e-03]]),\n",
       " 'b': -1.656699030651015}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
