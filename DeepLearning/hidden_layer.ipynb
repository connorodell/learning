{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Create a network with one hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, layer_size=2):\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    w = []\n",
    "    for i in range(layer_size):\n",
    "        w.append(np.zeros((n_x, 1)))\n",
    "    w.append(np.zeros((layer_size, 1)))\n",
    "    b = np.array(np.linspace(-1, 1, num=layer_size+1))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3],[1,2,3]])\n",
    "Y = np.full((3,1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]])] [-1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize(X)\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Sigmoid/logistic function.\n",
    "    \n",
    "        x = numpy array or float'''\n",
    "    \n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y, yhat):\n",
    "    '''Loss function for logistic regression.\n",
    "    \n",
    "        y = true (binary) class\n",
    "        yhat = predicted class probability'''\n",
    "    \n",
    "    return -(y*np.log(yhat)+(1-y)*np.log(1-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(X, Y, w, b, layer_size=2):\n",
    "    '''Implement forward and backward propogation for simple logistic regression.\n",
    "    \n",
    "        X = n_x by m numpy array containg training data where n_x is the number of features and m is the number of samples\n",
    "        Y = m by 1 numpy array containing true sample classes\n",
    "        w = numpy array of model parameters\n",
    "        b = model bias array'''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    sum_vec = np.full((1,m), 1)\n",
    "    \n",
    "    # forward propogation\n",
    "    # hidden layer\n",
    "    hidden = []\n",
    "    for i in range(layer_size):\n",
    "        hidden.append(sigmoid(np.dot(X.T,w[i])+b[i]))\n",
    "    hidden = np.concatenate(hidden, axis=1)\n",
    "    #print(hidden.shape)\n",
    "    \n",
    "    # output layer\n",
    "    out = sigmoid(np.dot(hidden,w[-1])+b[-1])\n",
    "    \n",
    "    # backward propogation\n",
    "    dw = []\n",
    "    for i in range(layer_size):\n",
    "        dw.append((1/m)*np.dot(sum_vec, (out-Y)*w[-1][i]*hidden[:,i].reshape(m,1)*(1-hidden[:,i].reshape(m,1))*X.T))\n",
    "    dw.append((1/m)*np.dot(sum_vec, (out-Y)*hidden))\n",
    "    \n",
    "    db = []\n",
    "    for i in range(layer_size):\n",
    "        db.append((1/m)*np.sum((out-Y)*w[-1][i]*hidden[:,i].reshape(m,1)*(1-hidden[:,i].reshape(m,1))))\n",
    "    db.append((1/m)*np.sum(out-Y))\n",
    "    \n",
    "    gradient = dict({'dw': dw, 'db': db})\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = propogate(X, Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw': [array([[0., 0.]]),\n",
       "  array([[0., 0.]]),\n",
       "  array([[-0.07232949, -0.13447071]])],\n",
       " 'db': [0.0, 0.0, -0.2689414213699951]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, w, b, layer_size=2, iterations=2000, learning_rate=0.01):\n",
    "    '''Implement gradient descent fitting procedure.\n",
    "    \n",
    "        X = n_x by m array with samples in columns\n",
    "        Y = m by 1 array of true binary class values\n",
    "        w = array of model parameters\n",
    "        b = model bias array'''\n",
    "    \n",
    "    n_x = X.shape[0]    # number of features\n",
    "    m = X.shape[1]    # number of training samples\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        gradient = propogate(X, Y, w, b, layer_size=layer_size)\n",
    "        dw = gradient['dw']\n",
    "        db = gradient['db']\n",
    "        for i in range(layer_size):\n",
    "            w[i] = w[i] - learning_rate*dw[i].reshape(n_x,1)\n",
    "            b[i] = b[i] - learning_rate*db[i]\n",
    "        w[-1] = w[-1] - learning_rate*dw[-1].reshape(layer_size,1)\n",
    "        b[-1] = b[-1] - learning_rate*db[-1]\n",
    "    \n",
    "    parameters = dict({'w': w, 'b': b})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': [array([[0.16518403],\n",
       "         [0.16518403]]),\n",
       "  array([[0.28924198],\n",
       "         [0.28924198]]),\n",
       "  array([[0.50815683],\n",
       "         [0.96924372]])],\n",
       " 'b': array([-0.91686881,  0.15322198,  2.60037197])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(X, Y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_val(x):\n",
    "    '''Find the most likely class given the input probability x.'''\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# use numpy vectorization to apply class_val to an array\n",
    "vclass_val = np.vectorize(class_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b, layer_size=2):\n",
    "    '''Predict class using logistic regression model.\n",
    "    \n",
    "        X = array with n_x rows, samples in columns'''\n",
    "    \n",
    "    # hidden layer\n",
    "    hidden = []\n",
    "    for i in range(layer_size):\n",
    "        hidden.append(sigmoid(np.dot(X.T,w[i])+b[i]))\n",
    "    hidden = np.concatenate(hidden, axis=1)\n",
    "    \n",
    "    # output layer\n",
    "    out = sigmoid(np.dot(hidden,w[-1])+b[-1])\n",
    "    \n",
    "    return vclass_val(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogModel(X_train, Y_train, X_test, Y_test, layer_size=2, iterations=2000, learning_rate=0.01):\n",
    "    '''Build logistic regression model using training data and test on provided\n",
    "    testing data.\n",
    "    \n",
    "        X_train = n_x by m array with feature vectors in columns\n",
    "        Y_train = m by 1 array of true binary class for each training sample\n",
    "        X_test = n_x by any number array with feature vectors in columns\n",
    "        Y_test = column vector of true binary class for each test sample\n",
    "        iterations = positive integer, number of iterations for gradient descent\n",
    "        learning_rate = positive float, learning rate used for gradient descent\n",
    "        \n",
    "    Prints percentage of correct predictions on testing data using the model fit by gradient descent.\n",
    "    Returns model parameters.'''\n",
    "    \n",
    "    w, b = initialize(X_train, layer_size=layer_size)\n",
    "    \n",
    "    parameters = fit(X_train, Y_train, w, b, layer_size=layer_size, iterations=iterations, learning_rate=learning_rate)\n",
    "    w = parameters['w']\n",
    "    b = parameters['b']\n",
    "    \n",
    "    predictions = predict(X_test, w, b, layer_size=layer_size)\n",
    "    \n",
    "    model_accuracy = 100-np.average(np.abs(Y_test-predictions))*100\n",
    "    \n",
    "    print('Model accuracy: {:.4f}%'.format(model_accuracy))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 66.6667%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': [array([[0.22710844],\n",
       "         [0.39658963]]),\n",
       "  array([[0.28710373],\n",
       "         [0.55467815]]),\n",
       "  array([[-0.44534983],\n",
       "         [-0.78345754]])],\n",
       " 'b': array([-0.99513974, -0.08545368,  1.21543515])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1],[0],[1]]), np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1],[0],[1]])\n",
    "\n",
    "LogModel(X_train, Y_train, X_test, Y_test, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw': [array([[0.08261021, 0.16552093]]),\n",
       "  array([[0.07363465, 0.14954918]]),\n",
       "  array([[0.16197719, 0.27118063]])],\n",
       " 'db': [0.03896272928411102, 0.0314154078861343, 0.29951223854595815]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propogate(X_train, Y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': [array([[-0.16982817],\n",
       "         [-0.54385937]]),\n",
       "  array([[-0.24610115],\n",
       "         [-0.73762346]]),\n",
       "  array([[0.59105422],\n",
       "         [1.10245521]])],\n",
       " 'b': array([-0.94309061,  0.05547752,  0.36230521])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(X_train, Y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '/Users/connorodell/Documents/Data_Science/learning/exercise_03_train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), train_df['y'], test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "# run new model\n",
    "# note that samples are in rows of X_train and X_test, so the transposes are fed into the models\n",
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': [array([[ 0.0453657 ],\n",
       "         [-0.07802928],\n",
       "         [-0.06925341],\n",
       "         [ 0.07898884],\n",
       "         [ 0.0074387 ],\n",
       "         [ 0.05845059],\n",
       "         [-0.00113127],\n",
       "         [-0.00445091],\n",
       "         [ 0.00860571],\n",
       "         [-0.00560125],\n",
       "         [ 0.06763096],\n",
       "         [ 0.00282689],\n",
       "         [ 0.01105655],\n",
       "         [-0.00404737],\n",
       "         [ 0.00541511],\n",
       "         [ 0.00475953],\n",
       "         [ 0.00372391],\n",
       "         [ 0.00542627],\n",
       "         [-0.00199364],\n",
       "         [-0.00551417],\n",
       "         [ 0.05904367],\n",
       "         [ 0.07177388],\n",
       "         [ 0.07340211],\n",
       "         [-0.00522769],\n",
       "         [-0.0013216 ],\n",
       "         [ 0.00217425],\n",
       "         [-0.00435343],\n",
       "         [ 0.00491317],\n",
       "         [-0.0024353 ],\n",
       "         [ 0.00616591],\n",
       "         [ 0.01263308],\n",
       "         [ 0.00644607],\n",
       "         [-0.00139472],\n",
       "         [-0.07094399],\n",
       "         [-0.00074028],\n",
       "         [ 0.15146677],\n",
       "         [-0.00885969],\n",
       "         [ 0.00231525],\n",
       "         [-0.06928928],\n",
       "         [ 0.13768633],\n",
       "         [ 0.00484384],\n",
       "         [ 0.00575175],\n",
       "         [-0.04306904],\n",
       "         [ 0.06859167],\n",
       "         [-0.00403661],\n",
       "         [ 0.00074699],\n",
       "         [ 0.00512277],\n",
       "         [-0.00117734],\n",
       "         [ 0.06670442],\n",
       "         [-0.07178152],\n",
       "         [-0.00486371],\n",
       "         [-0.00751319],\n",
       "         [-0.00311464],\n",
       "         [-0.00130178],\n",
       "         [ 0.07015331],\n",
       "         [ 0.00501759],\n",
       "         [-0.14305537],\n",
       "         [ 0.00149343],\n",
       "         [-0.00289796],\n",
       "         [ 0.00364841],\n",
       "         [-0.00211341],\n",
       "         [-0.0773848 ],\n",
       "         [ 0.00199608],\n",
       "         [-0.00345404],\n",
       "         [ 0.06964782],\n",
       "         [ 0.00341283],\n",
       "         [-0.07050899],\n",
       "         [ 0.08070265],\n",
       "         [-0.0004875 ],\n",
       "         [-0.06288024],\n",
       "         [-0.06876178],\n",
       "         [-0.01028868],\n",
       "         [ 0.16123963],\n",
       "         [ 0.00403515],\n",
       "         [-0.00217118],\n",
       "         [-0.07226251],\n",
       "         [-0.06629722],\n",
       "         [-0.00440494],\n",
       "         [-0.00511626],\n",
       "         [ 0.0009884 ],\n",
       "         [-0.06900491],\n",
       "         [-0.00139409],\n",
       "         [-0.0614853 ],\n",
       "         [-0.00438474],\n",
       "         [-0.00152191],\n",
       "         [-0.00279133],\n",
       "         [ 0.00084728],\n",
       "         [-0.00049913],\n",
       "         [ 0.00027552],\n",
       "         [ 0.00687999],\n",
       "         [-0.0059962 ],\n",
       "         [ 0.00916989],\n",
       "         [ 0.06962238],\n",
       "         [-0.14402392],\n",
       "         [-0.00784122],\n",
       "         [-0.07554475],\n",
       "         [-0.002701  ],\n",
       "         [-0.00312505],\n",
       "         [ 0.00048161],\n",
       "         [ 0.00383288],\n",
       "         [ 0.00066126],\n",
       "         [-0.00131531],\n",
       "         [-0.00027206],\n",
       "         [-0.00580754],\n",
       "         [ 0.00055445],\n",
       "         [ 0.00525713],\n",
       "         [ 0.01019599],\n",
       "         [-0.01543779],\n",
       "         [ 0.05017715],\n",
       "         [-0.04088909],\n",
       "         [-0.03913852],\n",
       "         [-0.01688751],\n",
       "         [ 0.00655002],\n",
       "         [-0.01131224],\n",
       "         [-0.01147067],\n",
       "         [-0.00261059],\n",
       "         [ 0.0136485 ],\n",
       "         [ 0.01089374],\n",
       "         [-0.01544025],\n",
       "         [-0.00718426],\n",
       "         [-0.01537846],\n",
       "         [-0.01910894],\n",
       "         [-0.00573255],\n",
       "         [-0.00031703],\n",
       "         [-0.00154682],\n",
       "         [ 0.00311011]]),\n",
       "  array([[ 0.0453657 ],\n",
       "         [-0.07802928],\n",
       "         [-0.06925341],\n",
       "         [ 0.07898884],\n",
       "         [ 0.0074387 ],\n",
       "         [ 0.05845059],\n",
       "         [-0.00113127],\n",
       "         [-0.00445091],\n",
       "         [ 0.00860571],\n",
       "         [-0.00560125],\n",
       "         [ 0.06763096],\n",
       "         [ 0.00282689],\n",
       "         [ 0.01105655],\n",
       "         [-0.00404737],\n",
       "         [ 0.00541511],\n",
       "         [ 0.00475953],\n",
       "         [ 0.00372391],\n",
       "         [ 0.00542627],\n",
       "         [-0.00199364],\n",
       "         [-0.00551417],\n",
       "         [ 0.05904367],\n",
       "         [ 0.07177388],\n",
       "         [ 0.07340211],\n",
       "         [-0.00522769],\n",
       "         [-0.0013216 ],\n",
       "         [ 0.00217425],\n",
       "         [-0.00435343],\n",
       "         [ 0.00491317],\n",
       "         [-0.0024353 ],\n",
       "         [ 0.00616591],\n",
       "         [ 0.01263308],\n",
       "         [ 0.00644607],\n",
       "         [-0.00139472],\n",
       "         [-0.07094399],\n",
       "         [-0.00074028],\n",
       "         [ 0.15146677],\n",
       "         [-0.00885969],\n",
       "         [ 0.00231525],\n",
       "         [-0.06928928],\n",
       "         [ 0.13768633],\n",
       "         [ 0.00484384],\n",
       "         [ 0.00575175],\n",
       "         [-0.04306904],\n",
       "         [ 0.06859167],\n",
       "         [-0.00403661],\n",
       "         [ 0.00074699],\n",
       "         [ 0.00512277],\n",
       "         [-0.00117734],\n",
       "         [ 0.06670442],\n",
       "         [-0.07178152],\n",
       "         [-0.00486371],\n",
       "         [-0.00751319],\n",
       "         [-0.00311464],\n",
       "         [-0.00130178],\n",
       "         [ 0.07015331],\n",
       "         [ 0.00501759],\n",
       "         [-0.14305537],\n",
       "         [ 0.00149343],\n",
       "         [-0.00289796],\n",
       "         [ 0.00364841],\n",
       "         [-0.00211341],\n",
       "         [-0.0773848 ],\n",
       "         [ 0.00199608],\n",
       "         [-0.00345404],\n",
       "         [ 0.06964782],\n",
       "         [ 0.00341283],\n",
       "         [-0.07050899],\n",
       "         [ 0.08070265],\n",
       "         [-0.0004875 ],\n",
       "         [-0.06288024],\n",
       "         [-0.06876178],\n",
       "         [-0.01028868],\n",
       "         [ 0.16123963],\n",
       "         [ 0.00403515],\n",
       "         [-0.00217118],\n",
       "         [-0.07226251],\n",
       "         [-0.06629722],\n",
       "         [-0.00440494],\n",
       "         [-0.00511626],\n",
       "         [ 0.0009884 ],\n",
       "         [-0.06900491],\n",
       "         [-0.00139409],\n",
       "         [-0.0614853 ],\n",
       "         [-0.00438474],\n",
       "         [-0.00152191],\n",
       "         [-0.00279133],\n",
       "         [ 0.00084728],\n",
       "         [-0.00049913],\n",
       "         [ 0.00027552],\n",
       "         [ 0.00687999],\n",
       "         [-0.0059962 ],\n",
       "         [ 0.00916989],\n",
       "         [ 0.06962238],\n",
       "         [-0.14402392],\n",
       "         [-0.00784122],\n",
       "         [-0.07554475],\n",
       "         [-0.002701  ],\n",
       "         [-0.00312505],\n",
       "         [ 0.00048161],\n",
       "         [ 0.00383288],\n",
       "         [ 0.00066126],\n",
       "         [-0.00131531],\n",
       "         [-0.00027206],\n",
       "         [-0.00580754],\n",
       "         [ 0.00055445],\n",
       "         [ 0.00525713],\n",
       "         [ 0.01019599],\n",
       "         [-0.01543779],\n",
       "         [ 0.05017715],\n",
       "         [-0.04088909],\n",
       "         [-0.03913852],\n",
       "         [-0.01688751],\n",
       "         [ 0.00655002],\n",
       "         [-0.01131224],\n",
       "         [-0.01147067],\n",
       "         [-0.00261059],\n",
       "         [ 0.0136485 ],\n",
       "         [ 0.01089374],\n",
       "         [-0.01544025],\n",
       "         [-0.00718426],\n",
       "         [-0.01537846],\n",
       "         [-0.01910894],\n",
       "         [-0.00573255],\n",
       "         [-0.00031703],\n",
       "         [-0.00154682],\n",
       "         [ 0.00311011]]),\n",
       "  array([[-0.65232034],\n",
       "         [-0.65232034]])],\n",
       " 'b': array([ 0.04641129,  0.04641129, -0.79520835])}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test, layer_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden neurons =  2\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  3\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  4\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  5\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  6\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  7\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  8\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  9\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  10\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  11\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  12\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  13\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  14\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  15\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  16\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  17\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  18\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  19\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  20\n",
      "Model accuracy: 79.6837%\n",
      "Number of hidden neurons =  21\n",
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('Number of hidden neurons = ', i+2)\n",
    "    params = LogModel(X_train.T, Y_train, X_test.T, Y_test, layer_size=i+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The program I wrote works in the sense that it runs without any errors\n",
    "* By the results above, it is clear that there is something wrong with my network, i.e., adding neurons has no impact on model accuracy.\n",
    "* Let's continue on to the next week of the course to see if I can find out how to fix my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test, layer_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Increasing the number of neurons in the hidden layer doesn't seem to have any effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31363, 126)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test, layer_size=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "params = LogModel(X_train.T, Y_train, X_test.T, Y_test, layer_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
