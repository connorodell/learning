{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an L-layer neural network (classification)\n",
    "\n",
    "Since the goal is to learn the details of neural networks, this model is implemented using numpy matrix operations. We also have added l2 and dropout regularization options.\n",
    "\n",
    "## Model parameters\n",
    "\n",
    "* X = $n_x$ by m array where training vectors are in columns\n",
    "* Y = 1 by m array of true class values\n",
    "* $n_x$ = number of predictors\n",
    "* m = number of training examples\n",
    "* L = number of layers\n",
    "* n = list containing number of neurons per layer, n[0] = $n_x$\n",
    "* Wi = n[i] by n[i-1] array of model parameters for layer i\n",
    "* bi = n[i] by i array of bias parameters for layer i\n",
    "* dWi = n[i] by n[i-1] array of partial derivatives of the cost function with respect to the corresponding parameters of Wi\n",
    "* dbi = n[i] by i array of partial derivatives of the cost function with respect to the corresponding bias parameters\n",
    "* regularization = None or 'l2'\n",
    "* lambd = regularization parameter for l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function for output vector\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function for hidden layers\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the relu function. We keep the shape of the input array for vectorization purposes. Strictly\n",
    "# speaking, the total derivative should be a (much larger) diagonal matrix\n",
    "def der_relu(z):\n",
    "    return (z>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    '''Predict binary class value given input X and model parameters. The activation function\n",
    "    for each hidden layer is a ReLU and the output activation function is a sigmoind. Since this \n",
    "    function is only used for model predictions, no intermediate values of the model are cached\n",
    "    or returned to the user.\n",
    "    \n",
    "    Input:\n",
    "        X = n_x by m array of feature vectors\n",
    "        parameters = dictionary of parameters Wi and bi for the neural network\n",
    "    \n",
    "    Output:\n",
    "        A = 1 by m array of predicted class values.'''\n",
    "    \n",
    "    # Calculate the number of layers in the network\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # hidden layer computations\n",
    "    A = X\n",
    "    for i in range(1, L):\n",
    "        A = relu(np.dot(parameters['W'+str(i)], A) + parameters['b'+str(i)])\n",
    "        \n",
    "    # output layer\n",
    "    A = sigmoid(np.dot(parameters['W'+str(L)], A) + parameters['b'+str(L)])\n",
    "    \n",
    "    # predict class\n",
    "    A = (A >= 0.5)\n",
    "    \n",
    "    return A.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n):\n",
    "    '''Initialize model parameters using the input specification. We use He initialization since we have ReLU\n",
    "    activation functions.\n",
    "    \n",
    "    Input:\n",
    "        n = length L+1 list where n[i] is the number of neurons in layer i\n",
    "        \n",
    "    Output:\n",
    "        parameters = dictionary containing initialized model parameters Wi and bi'''\n",
    "    \n",
    "    parameters = dict()\n",
    "    for i in range(1, len(n)):\n",
    "        parameters['W'+str(i)] = np.random.randn(n[i], n[i-1])*(2/np.sqrt(n[i-1]))\n",
    "        parameters['b'+str(i)] = np.zeros((n[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(X, Y, parameters, regularization=None):\n",
    "    '''Perform forward and backward propogation.\n",
    "    \n",
    "    Input:\n",
    "        X = n_x by m array containing training examples\n",
    "        Y = 1 by m array containing true binary class value for training examples\n",
    "        parameters = dictionary of model parameters Wi and bi\n",
    "        \n",
    "    Output:\n",
    "        gradient = dictionary of gradient arrays dWi and dbi corresponding to model parameters Wi and bi'''\n",
    "    \n",
    "    m = X.shape[1]    # infer the number of training examples from X\n",
    "    L = len(parameters)//2    # infer the number of layers from the number of parameters\n",
    "    sum_vec = np.full((m, 1), 1)    # vector used in gradient calculations\n",
    "    cache = dict()    # cache for results of intermediate steps\n",
    "    \n",
    "    # forward propogation\n",
    "    \n",
    "    # hidden layer computations\n",
    "    A = X    # A stores the activation of the previous layer, initializes to training vectors\n",
    "    cache['A0'] = A\n",
    "    for i in range(1, L):\n",
    "        Z = np.dot(parameters['W'+str(i)], A) + parameters['b'+str(i)]    # linear step\n",
    "        A = relu(Z)    # activation\n",
    "        cache['Z'+str(i)] = Z    # store intermediate values for use in gradient calcululation\n",
    "        cache['A'+str(i)] = A\n",
    "        \n",
    "    # output layer\n",
    "    Z = np.dot(parameters['W'+str(L)], A) + parameters['b'+str(L)]    # linear step\n",
    "    A = sigmoid(Z)    # activation\n",
    "    cache['Z'+str(L)] = Z    # store intermediate values for use in gradient calculation\n",
    "    cache['A'+str(L)] = A\n",
    "    \n",
    "    #backward propogation\n",
    "    gradient = dict()\n",
    "    \n",
    "    # output layer\n",
    "    dA = (1/m)*(A-Y)    # tracking matrix for chain rule\n",
    "    gradient['dW'+str(L)] = np.dot(dA, cache['A'+str(L-1)].T)    # dWL\n",
    "    gradient['db'+str(L)] = np.dot(dA, sum_vec)    # dbL\n",
    "    \n",
    "    # hidden layers\n",
    "    for i in reversed(range(1, L)):\n",
    "        dA = np.dot(dA.T, parameters['W'+str(i+1)])*der_relu(cache['Z'+str(i)].T)    # update dA. See page 3 of notes for derivation/proof\n",
    "        dA = dA.T\n",
    "        gradient['dW'+str(i)] = np.dot(dA, cache['A'+str(i-1)].T)    # dWi\n",
    "        gradient['db'+str(i)] = np.dot(dA, sum_vec)    # dbi\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Yhat, Y, eps=0.000000001, regularization=None, lambd=0, parameters=None):\n",
    "    '''Calculate cost of prediction Yhat. The small value eps is used to prevent log(0) error.\n",
    "    \n",
    "    Input:\n",
    "        Yhat = 1 by m array of class probabilities\n",
    "        Y = 1 by m array of true binary class\n",
    "        eps = small value to prevent log(0) error\n",
    "        regularization = None or l2\n",
    "        lambd = l2 regularization constant\n",
    "        parameters = dictionary of model parameters\n",
    "        \n",
    "    Output:\n",
    "        cost = cost of the given prediction Yhat'''\n",
    "    \n",
    "    m = Yhat.shape[1]    # infer the number of training examples\n",
    "    \n",
    "    if regularization == 'l2':\n",
    "        L = len(parameters)//2   # infer the number of layers\n",
    "        l2_sum = 0   # sum squares of parameters\n",
    "        for i in range(1, L+1):\n",
    "            l2_sum += np.sum(parameters['W'+str(i)]**2)\n",
    "        return -(1/m)*np.sum(Y*np.log(Yhat + eps)+(1-Y)*np.log(1-Yhat + eps)) + (lambd/(2*m))*l2_sum\n",
    "    else:\n",
    "        return -(1/m)*np.sum(Y*np.log(Yhat + eps)+(1-Y)*np.log(1-Yhat + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, parameters, learning_rate=0.01, iterations=2000, regularization=None, lambd=0, print_cost=False):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradient = propogate(X, Y, parameters, regularization=regularization)\n",
    "        \n",
    "        # print cost every 1000 iterations\n",
    "        if (i%1000 == 0) and (print_cost == True):\n",
    "            current_cost = cost(predict(X, parameters), Y, regularization=regularization, lambd=lambd, parameters=parameters)\n",
    "            print('Cost after {} iterations: {:.12f}'.format(i, current_cost))\n",
    "        \n",
    "        # update parameters\n",
    "        if regularization == 'l2':\n",
    "            for i in range(1, L+1):\n",
    "                parameters['W'+str(i)] = (1-lambd/m)*parameters['W'+str(i)] - learning_rate*gradient['dW'+str(i)]\n",
    "                parameters['b'+str(i)] = parameters['b'+str(i)] - learning_rate*gradient['db'+str(i)]\n",
    "        else:\n",
    "            for i in range(1, L+1):\n",
    "                parameters['W'+str(i)] = parameters['W'+str(i)] - learning_rate*gradient['dW'+str(i)]\n",
    "                parameters['b'+str(i)] = parameters['b'+str(i)] - learning_rate*gradient['db'+str(i)]\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X_train, Y_train, X_test, Y_test, n, learning_rate=0.01, iterations=2000, regularization=None, lambd=0, print_cost=False):\n",
    "    \n",
    "    # initialize model parameters\n",
    "    parameters = initialize(n)\n",
    "    \n",
    "    # fit model\n",
    "    parameters = fit(X_train, Y_train, parameters, learning_rate=learning_rate, iterations=iterations, regularization=regularization, lambd=lambd, print_cost=print_cost)\n",
    "    \n",
    "    train_predictions = predict(X_train, parameters)\n",
    "    train_accuracy = 100-np.average(np.abs(Y_train-train_predictions))*100\n",
    "    \n",
    "    test_predictions = predict(X_test, parameters)\n",
    "    test_accuracy = 100-np.average(np.abs(Y_test-test_predictions))*100\n",
    "    \n",
    "    print('Training set accuracy: {:.4f}%'.format(train_accuracy))\n",
    "    print('Test set accuracy: {:.4f}%'.format(test_accuracy))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the functions with a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X = np.random.randn(4,5)\n",
    "Y = np.array([[1, 1, 0, 0, 1]])\n",
    "n = [4, 6, 5, 4, 3, 7, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32929271, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.00885492, 0.        , 0.0689359 , 0.        ],\n",
       "       [0.97381624, 0.        , 0.        , 0.28006397, 0.32178048],\n",
       "       [0.        , 0.68706224, 0.        , 0.48878788, 0.05322104]])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38588934, 0.58158727, 0.39221099, 0.37108738, 0.46023881],\n",
       "       [0.34169732, 0.732796  , 0.25808524, 0.51722715, 0.30479082],\n",
       "       [0.7258795 , 0.15544154, 0.35515476, 0.56956191, 0.57975811],\n",
       "       [0.38838279, 0.66531309, 0.44656568, 0.61982085, 0.51330212]])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = propogate(X, Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['b4'] - 0.001*gradient['db4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 8.396865708003\n",
      "Cost after 1000 iterations: 8.290026540589\n"
     ]
    }
   ],
   "source": [
    "parameters = fit(X, Y, parameters, regularization='l2', lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = predict(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.28931115660599"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(Yhat, Y, regularization='l2', lambd=0.01, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 9.226144710205\n",
      "Cost after 1000 iterations: 4.243249766608\n",
      "Cost after 2000 iterations: 8.289306334179\n",
      "Cost after 3000 iterations: 8.289306334179\n",
      "Cost after 4000 iterations: 8.289306334179\n",
      "Cost after 5000 iterations: 8.289306334179\n",
      "Cost after 6000 iterations: 8.289306334179\n",
      "Cost after 7000 iterations: 8.289306334179\n",
      "Cost after 8000 iterations: 8.289306334179\n",
      "Cost after 9000 iterations: 8.289306334179\n",
      "Cost after 10000 iterations: 8.289306334179\n",
      "Cost after 11000 iterations: 8.289306334179\n",
      "Cost after 12000 iterations: 8.289306334179\n",
      "Cost after 13000 iterations: 8.289306334179\n",
      "Cost after 14000 iterations: 8.289306334179\n",
      "Cost after 15000 iterations: 8.289306334179\n",
      "Cost after 16000 iterations: 8.289306334179\n",
      "Cost after 17000 iterations: 8.289306334179\n",
      "Cost after 18000 iterations: 8.289306334179\n",
      "Cost after 19000 iterations: 8.289306334179\n",
      "Training set accuracy: 60.0000%\n",
      "Test set accuracy: 60.0000%\n"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(X, Y, X, Y, n, learning_rate=0.5, iterations=20000, regularization='l2', lambd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.54593054e-210, -2.57532995e-210, -2.60437688e-210,\n",
       "        -4.78209990e-210],\n",
       "       [-2.54282026e-209, -2.89467025e-209, -4.91050848e-209,\n",
       "        -2.99867850e-209],\n",
       "       [ 1.40403499e-209, -5.52505015e-210, -5.58606812e-210,\n",
       "        -1.02583736e-209],\n",
       "       [ 1.47818700e-220,  3.99197220e-222,  8.84529653e-222,\n",
       "        -5.14954609e-221],\n",
       "       [-2.26273085e-209, -2.63616066e-209, -4.14555713e-209,\n",
       "        -2.69758290e-209],\n",
       "       [ 2.46643797e-209, -9.70424321e-210, -9.81406194e-210,\n",
       "        -1.80184342e-209]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['W1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an 'or' network\n",
    "\n",
    "Test the L layer model by building a simple network to represent an or gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 0, 0], [1, 0, 1, 0]])\n",
    "Y = np.array([[1, 1, 1, 0]])\n",
    "n = [2, 4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 5.415018940264\n",
      "Cost after 1000 iterations: 0.092390400668\n",
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# model with single hidden layer of four neurons\n",
    "parameters = L_layer_model(X, Y, X, Y, n, learning_rate=0.9, iterations=2000, regularization='l2', lambd=0.1, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# add a second hidden layer\n",
    "n = [2, 4, 2, 1]\n",
    "parameters = L_layer_model(X, Y, X, Y, n, learning_rate=0.1, iterations=20000, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# Simple networks are best in this situation\n",
    "n = [2, 2, 1]\n",
    "parameters = L_layer_model(X, Y, X, Y, n, learning_rate=0.1, iterations=20000, regularization='l2', lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([[1], [0]]), parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above tests successfully, it appears that the L layer model is functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the L layer model on interview data set\n",
    "\n",
    "We import an interview data set to investigate the performance of an L layer neural network on a larger data set with more predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '' # insert file path here\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 127)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])\n",
    "\n",
    "# Take the first 1000 samples for some quick tests\n",
    "train_df = train_df.iloc[:1000,:]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 127 data columns in the data set. The column labeled 'y' contains the true binary class of each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), (train_df['y']), test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 126)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with one hidden layer of three neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 17.174500831276\n",
      "Cost after 1000 iterations: 0.725473019850\n",
      "Cost after 2000 iterations: 0.621933756145\n",
      "Cost after 3000 iterations: 0.596062268569\n",
      "Cost after 4000 iterations: 0.596077053197\n",
      "Cost after 5000 iterations: 0.596083724827\n",
      "Cost after 6000 iterations: 0.596086945799\n",
      "Cost after 7000 iterations: 0.596087305082\n",
      "Cost after 8000 iterations: 0.596086729450\n",
      "Cost after 9000 iterations: 0.440663387838\n",
      "Cost after 10000 iterations: 0.440663100800\n",
      "Cost after 11000 iterations: 0.414758287677\n",
      "Cost after 12000 iterations: 0.414757619184\n",
      "Cost after 13000 iterations: 0.388852851078\n",
      "Cost after 14000 iterations: 0.337044893018\n",
      "Cost after 15000 iterations: 0.311142009118\n",
      "Cost after 16000 iterations: 0.311142667390\n",
      "Cost after 17000 iterations: 0.311142842948\n",
      "Cost after 18000 iterations: 0.311142726611\n",
      "Cost after 19000 iterations: 0.311142472852\n",
      "Cost after 20000 iterations: 0.311142135066\n",
      "Cost after 21000 iterations: 0.311141810185\n",
      "Cost after 22000 iterations: 0.311141524736\n",
      "Cost after 23000 iterations: 0.311141284040\n",
      "Cost after 24000 iterations: 0.311141094467\n",
      "Cost after 25000 iterations: 0.311140946984\n",
      "Cost after 26000 iterations: 0.311140822063\n",
      "Cost after 27000 iterations: 0.311140724932\n",
      "Cost after 28000 iterations: 0.311140650693\n",
      "Cost after 29000 iterations: 0.311140588120\n",
      "Training set accuracy: 98.5000%\n",
      "Test set accuracy: 84.5000%\n"
     ]
    }
   ],
   "source": [
    "# note that samples are in rows of train_df, so the transposes are fed into the models\n",
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 3, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "* Training accuracy = 98.5%\n",
    "* Test accuracy = 84.5%\n",
    "We appear to be overfitting the model to the data. We can try increasing the regularization to prevent this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 5.470268891816\n",
      "Cost after 1000 iterations: 1.271012331030\n",
      "Cost after 2000 iterations: 1.141639926927\n",
      "Cost after 3000 iterations: 0.960348981523\n",
      "Cost after 4000 iterations: 1.012160190017\n",
      "Cost after 5000 iterations: 1.012166777467\n",
      "Cost after 6000 iterations: 1.012167700083\n",
      "Cost after 7000 iterations: 1.038072339403\n",
      "Cost after 8000 iterations: 1.038065651807\n",
      "Cost after 9000 iterations: 1.038066926702\n",
      "Cost after 10000 iterations: 1.038067147211\n",
      "Cost after 11000 iterations: 1.038067082628\n",
      "Cost after 12000 iterations: 1.038067374484\n",
      "Cost after 13000 iterations: 1.038067305621\n",
      "Cost after 14000 iterations: 1.038067353607\n",
      "Cost after 15000 iterations: 1.038067585201\n",
      "Cost after 16000 iterations: 1.038067236351\n",
      "Cost after 17000 iterations: 1.038067399130\n",
      "Cost after 18000 iterations: 1.038067140730\n",
      "Cost after 19000 iterations: 1.038067292885\n",
      "Cost after 20000 iterations: 1.038067564015\n",
      "Cost after 21000 iterations: 1.038067381729\n",
      "Cost after 22000 iterations: 1.038067547840\n",
      "Cost after 23000 iterations: 1.038067528085\n",
      "Cost after 24000 iterations: 1.038067459728\n",
      "Cost after 25000 iterations: 1.038067549786\n",
      "Cost after 26000 iterations: 1.038067698346\n",
      "Cost after 27000 iterations: 1.038067470400\n",
      "Cost after 28000 iterations: 1.038067443596\n",
      "Cost after 29000 iterations: 1.038067508824\n",
      "Training set accuracy: 95.0000%\n",
      "Test set accuracy: 87.0000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 3, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.5, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results indicate that increasing the value of lambd reduces the amount of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 8.218089347854\n",
      "Cost after 1000 iterations: 1.478261382085\n",
      "Cost after 2000 iterations: 1.581859493972\n",
      "Cost after 3000 iterations: 1.581828718192\n",
      "Cost after 4000 iterations: 1.581813130745\n",
      "Cost after 5000 iterations: 1.581816937729\n",
      "Cost after 6000 iterations: 1.581815329419\n",
      "Cost after 7000 iterations: 1.581811210604\n",
      "Cost after 8000 iterations: 1.581811317183\n",
      "Cost after 9000 iterations: 1.581811568895\n",
      "Cost after 10000 iterations: 1.581811607957\n",
      "Cost after 11000 iterations: 1.555907457306\n",
      "Cost after 12000 iterations: 1.555907716240\n",
      "Cost after 13000 iterations: 1.581812039888\n",
      "Cost after 14000 iterations: 1.581811839860\n",
      "Cost after 15000 iterations: 1.555907672412\n",
      "Cost after 16000 iterations: 1.555907825443\n",
      "Cost after 17000 iterations: 1.555907793317\n",
      "Cost after 18000 iterations: 1.555907786853\n",
      "Cost after 19000 iterations: 1.555907804617\n",
      "Cost after 20000 iterations: 1.581811992176\n",
      "Cost after 21000 iterations: 1.555907859592\n",
      "Cost after 22000 iterations: 1.581811751734\n",
      "Cost after 23000 iterations: 1.581811955989\n",
      "Cost after 24000 iterations: 1.555907775267\n",
      "Cost after 25000 iterations: 1.581811709442\n",
      "Cost after 26000 iterations: 1.581811678957\n",
      "Cost after 27000 iterations: 1.555907744913\n",
      "Cost after 28000 iterations: 1.581811890235\n",
      "Cost after 29000 iterations: 1.581811965221\n",
      "Training set accuracy: 92.5000%\n",
      "Test set accuracy: 86.5000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 3, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.6, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results indicate that increasing the value of lambd further does not necessarily improve performance any more.\n",
    "\n",
    "We can try models with more hidden layers/neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 97.6250%\n",
      "Test set accuracy: 86.0000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 6, 3, 1], learning_rate=0.1, iterations=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 94.5000%\n",
      "Test set accuracy: 80.0000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 3, 1], learning_rate=0.1, iterations=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 89.5000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 3, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the neural networks to simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 91.8750%\n",
      "Test set accuracy: 90.0000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=0.01, iterations=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 92.2500%\n",
      "Test set accuracy: 89.5000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=0.1, iterations=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 92.6250%\n",
      "Test set accuracy: 88.0000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=1, iterations=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 91.6250%\n",
      "Test set accuracy: 90.5000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=0.005, iterations=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that neural networks don't have performance gains over simple logistic regression when the amount of available data is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the amount of data to 10000 samples to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '/Users/connorodell/Documents/Data_Science/learning/exercise_03_train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 127)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])\n",
    "\n",
    "train_df = train_df.iloc[:10000,:]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), (train_df['y']), test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 126)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 99.8000%\n",
      "Test set accuracy: 97.2000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 3, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 97.2000%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 3, 1], learning_rate=0.1, iterations=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more data, test accuracy has improved on the order of 10%.\n",
    "\n",
    "We can see if the logistic regression model has similary performance gains with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 89.0625%\n",
      "Test set accuracy: 89.1500%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=0.1, iterations=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results indicate that more data does not improve the logistic regression model. We can interpret this to mean that the model has too much bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the models with the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '/Users/connorodell/Documents/Data_Science/learning/exercise_03_train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])\n",
    "\n",
    "#train_df = train_df.iloc[:10000,:]\n",
    "#train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), (train_df['y']), test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test various hyperparameters to evaluate relative performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 13.580543559158\n",
      "Cost after 1000 iterations: 0.893377214231\n",
      "Training set accuracy: 98.4823%\n",
      "Test set accuracy: 97.0157%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 3, 1], learning_rate=0.1, iterations=2000, regularization='l2', lambd=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 9.930490139564\n",
      "Cost after 1000 iterations: 1.192002408580\n",
      "Training set accuracy: 97.9594%\n",
      "Test set accuracy: 96.3398%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=2000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 13.846125549303\n",
      "Cost after 1000 iterations: 1.180769569909\n",
      "Cost after 2000 iterations: 0.399096149436\n",
      "Cost after 3000 iterations: 0.171796355148\n",
      "Cost after 4000 iterations: 0.100434791825\n",
      "Cost after 5000 iterations: 0.075986848835\n",
      "Cost after 6000 iterations: 0.058807213220\n",
      "Cost after 7000 iterations: 0.049556640197\n",
      "Cost after 8000 iterations: 0.040306067173\n",
      "Cost after 9000 iterations: 0.035020025446\n",
      "Cost after 10000 iterations: 0.031055494150\n",
      "Cost after 11000 iterations: 0.027751718070\n",
      "Cost after 12000 iterations: 0.025108697206\n",
      "Cost after 13000 iterations: 0.018501145047\n",
      "Cost after 14000 iterations: 0.015858124183\n",
      "Cost after 15000 iterations: 0.013215103319\n",
      "Cost after 16000 iterations: 0.007268306375\n",
      "Cost after 17000 iterations: 0.004625285512\n",
      "Cost after 18000 iterations: 0.003964530296\n",
      "Cost after 19000 iterations: 0.002643019864\n",
      "Cost after 20000 iterations: 0.001982264648\n",
      "Cost after 21000 iterations: 0.000660754216\n",
      "Cost after 22000 iterations: 0.000660754216\n",
      "Cost after 23000 iterations: -0.000000001000\n",
      "Cost after 24000 iterations: -0.000000001000\n",
      "Cost after 25000 iterations: -0.000000001000\n",
      "Cost after 26000 iterations: -0.000000001000\n",
      "Cost after 27000 iterations: -0.000000001000\n",
      "Cost after 28000 iterations: -0.000000001000\n",
      "Cost after 29000 iterations: -0.000000001000\n",
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 98.3038%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 4.545995884759\n",
      "Cost after 1000 iterations: 0.905234644856\n",
      "Cost after 2000 iterations: 0.291393049235\n",
      "Cost after 3000 iterations: 0.117614427440\n",
      "Cost after 4000 iterations: 0.066075520595\n",
      "Cost after 5000 iterations: 0.042288332821\n",
      "Cost after 6000 iterations: 0.031055494150\n",
      "Cost after 7000 iterations: 0.023126431558\n",
      "Cost after 8000 iterations: 0.015858124183\n",
      "Cost after 9000 iterations: 0.011893592887\n",
      "Cost after 10000 iterations: 0.005946795944\n",
      "Cost after 11000 iterations: 0.003303775080\n",
      "Cost after 12000 iterations: 0.002643019864\n",
      "Cost after 13000 iterations: 0.001982264648\n",
      "Cost after 14000 iterations: 0.001982264648\n",
      "Cost after 15000 iterations: 0.001321509432\n",
      "Cost after 16000 iterations: 0.000660754216\n",
      "Cost after 17000 iterations: -0.000000001000\n",
      "Cost after 18000 iterations: -0.000000001000\n",
      "Cost after 19000 iterations: -0.000000001000\n",
      "Cost after 20000 iterations: -0.000000001000\n",
      "Cost after 21000 iterations: -0.000000001000\n",
      "Cost after 22000 iterations: -0.000000001000\n",
      "Cost after 23000 iterations: -0.000000001000\n",
      "Cost after 24000 iterations: -0.000000001000\n",
      "Cost after 25000 iterations: -0.000000001000\n",
      "Cost after 26000 iterations: -0.000000001000\n",
      "Cost after 27000 iterations: -0.000000001000\n",
      "Cost after 28000 iterations: -0.000000001000\n",
      "Cost after 29000 iterations: -0.000000001000\n",
      "Training set accuracy: 100.0000%\n",
      "Test set accuracy: 98.2528%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 10, 10, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 4.869765940577\n",
      "Cost after 1000 iterations: 0.807442872895\n",
      "Cost after 2000 iterations: 0.227299793288\n",
      "Cost after 3000 iterations: 0.124221979599\n",
      "Cost after 4000 iterations: 0.099113281393\n",
      "Cost after 5000 iterations: 0.090523463586\n",
      "Cost after 6000 iterations: 0.089862708370\n",
      "Cost after 7000 iterations: 0.089201953154\n",
      "Cost after 8000 iterations: 0.087880442722\n",
      "Cost after 9000 iterations: 0.087219687506\n",
      "Cost after 10000 iterations: 0.086558932290\n",
      "Cost after 11000 iterations: 0.085237421858\n",
      "Cost after 12000 iterations: 0.082594400994\n",
      "Cost after 13000 iterations: 0.081933645778\n",
      "Cost after 14000 iterations: 0.081933645778\n",
      "Cost after 15000 iterations: 0.082594400994\n",
      "Cost after 16000 iterations: 0.081933645778\n",
      "Cost after 17000 iterations: 0.080612135346\n",
      "Cost after 18000 iterations: 0.079290624914\n",
      "Cost after 19000 iterations: 0.079290624914\n",
      "Cost after 20000 iterations: 0.079290624914\n",
      "Cost after 21000 iterations: 0.078629869698\n",
      "Cost after 22000 iterations: 0.077969114483\n",
      "Cost after 23000 iterations: 0.077969114483\n",
      "Cost after 24000 iterations: 0.077969114483\n",
      "Cost after 25000 iterations: 0.077308359267\n",
      "Cost after 26000 iterations: 0.077308359267\n",
      "Cost after 27000 iterations: 0.077308359267\n",
      "Cost after 28000 iterations: 0.078629869698\n",
      "Cost after 29000 iterations: 0.078629869698\n",
      "Training set accuracy: 99.6206%\n",
      "Test set accuracy: 98.8012%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 16.477252819230\n",
      "Cost after 1000 iterations: 0.954791286053\n",
      "Cost after 2000 iterations: 0.243157918471\n",
      "Cost after 3000 iterations: 0.116292917008\n",
      "Cost after 4000 iterations: 0.099113281393\n",
      "Cost after 5000 iterations: 0.094487994881\n",
      "Cost after 6000 iterations: 0.089201953154\n",
      "Cost after 7000 iterations: 0.087219687506\n",
      "Cost after 8000 iterations: 0.086558932290\n",
      "Cost after 9000 iterations: 0.085898177074\n",
      "Cost after 10000 iterations: 0.084576666642\n",
      "Cost after 11000 iterations: 0.085237421858\n",
      "Cost after 12000 iterations: 0.084576666642\n",
      "Cost after 13000 iterations: 0.083915911426\n",
      "Cost after 14000 iterations: 0.082594400994\n",
      "Cost after 15000 iterations: 0.082594400994\n",
      "Cost after 16000 iterations: 0.082594400994\n",
      "Cost after 17000 iterations: 0.081933645778\n",
      "Cost after 18000 iterations: 0.081933645778\n",
      "Cost after 19000 iterations: 0.081933645778\n",
      "Cost after 20000 iterations: 0.081933645778\n",
      "Cost after 21000 iterations: 0.081272890562\n",
      "Cost after 22000 iterations: 0.081272890562\n",
      "Cost after 23000 iterations: 0.081272890562\n",
      "Cost after 24000 iterations: 0.080612135346\n",
      "Cost after 25000 iterations: 0.080612135346\n",
      "Cost after 26000 iterations: 0.080612135346\n",
      "Cost after 27000 iterations: 0.080612135346\n",
      "Cost after 28000 iterations: 0.080612135346\n",
      "Cost after 29000 iterations: 0.079951380130\n",
      "Training set accuracy: 99.6174%\n",
      "Test set accuracy: 98.7502%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 9.189122787264\n",
      "Cost after 1000 iterations: 0.864928576683\n",
      "Cost after 2000 iterations: 0.228621303720\n",
      "Cost after 3000 iterations: 0.122239713951\n",
      "Cost after 4000 iterations: 0.101095547041\n",
      "Cost after 5000 iterations: 0.092505729233\n",
      "Cost after 6000 iterations: 0.089862708370\n",
      "Cost after 7000 iterations: 0.089862708370\n",
      "Cost after 8000 iterations: 0.088541197938\n",
      "Cost after 9000 iterations: 0.087880442722\n",
      "Cost after 10000 iterations: 0.088541197938\n",
      "Cost after 11000 iterations: 0.087219687506\n",
      "Cost after 12000 iterations: 0.087219687506\n",
      "Cost after 13000 iterations: 0.085898177074\n",
      "Cost after 14000 iterations: 0.086558932290\n",
      "Cost after 15000 iterations: 0.084576666642\n",
      "Cost after 16000 iterations: 0.083255156210\n",
      "Cost after 17000 iterations: 0.083255156210\n",
      "Cost after 18000 iterations: 0.083915911426\n",
      "Cost after 19000 iterations: 0.083915911426\n",
      "Cost after 20000 iterations: 0.084576666642\n",
      "Cost after 21000 iterations: 0.084576666642\n",
      "Cost after 22000 iterations: 0.083915911426\n",
      "Cost after 23000 iterations: 0.083915911426\n",
      "Cost after 24000 iterations: 0.083915911426\n",
      "Cost after 25000 iterations: 0.083915911426\n",
      "Cost after 26000 iterations: 0.083915911426\n",
      "Cost after 27000 iterations: 0.083255156210\n",
      "Cost after 28000 iterations: 0.082594400994\n",
      "Cost after 29000 iterations: 0.081933645778\n",
      "Training set accuracy: 99.6078%\n",
      "Test set accuracy: 98.6736%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 6.873175755347\n",
      "Cost after 1000 iterations: 0.641593313691\n",
      "Cost after 2000 iterations: 0.180386172955\n",
      "Cost after 3000 iterations: 0.118275182656\n",
      "Cost after 4000 iterations: 0.103738567905\n",
      "Cost after 5000 iterations: 0.101095547041\n",
      "Cost after 6000 iterations: 0.096470260529\n",
      "Cost after 7000 iterations: 0.095809505313\n",
      "Cost after 8000 iterations: 0.095809505313\n",
      "Cost after 9000 iterations: 0.095148750097\n",
      "Cost after 10000 iterations: 0.094487994881\n",
      "Cost after 11000 iterations: 0.091844974018\n",
      "Cost after 12000 iterations: 0.091184218802\n",
      "Cost after 13000 iterations: 0.090523463586\n",
      "Cost after 14000 iterations: 0.090523463586\n",
      "Cost after 15000 iterations: 0.091184218802\n",
      "Cost after 16000 iterations: 0.091184218802\n",
      "Cost after 17000 iterations: 0.091184218802\n",
      "Cost after 18000 iterations: 0.089862708370\n",
      "Cost after 19000 iterations: 0.089201953154\n",
      "Cost after 20000 iterations: 0.088541197938\n",
      "Cost after 21000 iterations: 0.088541197938\n",
      "Cost after 22000 iterations: 0.088541197938\n",
      "Cost after 23000 iterations: 0.088541197938\n",
      "Cost after 24000 iterations: 0.088541197938\n",
      "Cost after 25000 iterations: 0.088541197938\n",
      "Cost after 26000 iterations: 0.089201953154\n",
      "Cost after 27000 iterations: 0.088541197938\n",
      "Cost after 28000 iterations: 0.088541197938\n",
      "Cost after 29000 iterations: 0.088541197938\n",
      "Training set accuracy: 99.5727%\n",
      "Test set accuracy: 98.7884%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 12.787595693346\n",
      "Cost after 1000 iterations: 0.708990345718\n",
      "Cost after 2000 iterations: 0.179725417739\n",
      "Cost after 3000 iterations: 0.119596693088\n",
      "Cost after 4000 iterations: 0.109024609632\n",
      "Cost after 5000 iterations: 0.107042343984\n",
      "Cost after 6000 iterations: 0.105720833553\n",
      "Cost after 7000 iterations: 0.103738567905\n",
      "Cost after 8000 iterations: 0.102417057473\n",
      "Cost after 9000 iterations: 0.101095547041\n",
      "Cost after 10000 iterations: 0.101756302257\n",
      "Cost after 11000 iterations: 0.101756302257\n",
      "Cost after 12000 iterations: 0.100434791825\n",
      "Cost after 13000 iterations: 0.100434791825\n",
      "Cost after 14000 iterations: 0.100434791825\n",
      "Cost after 15000 iterations: 0.099774036609\n",
      "Cost after 16000 iterations: 0.099113281393\n",
      "Cost after 17000 iterations: 0.099774036609\n",
      "Cost after 18000 iterations: 0.101756302257\n",
      "Cost after 19000 iterations: 0.101095547041\n",
      "Cost after 20000 iterations: 0.101095547041\n",
      "Cost after 21000 iterations: 0.101095547041\n",
      "Cost after 22000 iterations: 0.101095547041\n",
      "Cost after 23000 iterations: 0.100434791825\n",
      "Cost after 24000 iterations: 0.100434791825\n",
      "Cost after 25000 iterations: 0.100434791825\n",
      "Cost after 26000 iterations: 0.100434791825\n",
      "Cost after 27000 iterations: 0.100434791825\n",
      "Cost after 28000 iterations: 0.100434791825\n",
      "Cost after 29000 iterations: 0.100434791825\n",
      "Training set accuracy: 99.5154%\n",
      "Test set accuracy: 98.7119%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 16.392676151588\n",
      "Cost after 1000 iterations: 0.700400527911\n",
      "Cost after 2000 iterations: 0.171135599932\n",
      "Cost after 3000 iterations: 0.124221979599\n",
      "Cost after 4000 iterations: 0.119596693088\n",
      "Cost after 5000 iterations: 0.117614427440\n",
      "Cost after 6000 iterations: 0.112989140928\n",
      "Cost after 7000 iterations: 0.112989140928\n",
      "Cost after 8000 iterations: 0.111006875280\n",
      "Cost after 9000 iterations: 0.110346120064\n",
      "Cost after 10000 iterations: 0.109024609632\n",
      "Cost after 11000 iterations: 0.109024609632\n",
      "Cost after 12000 iterations: 0.108363854416\n",
      "Cost after 13000 iterations: 0.107042343984\n",
      "Cost after 14000 iterations: 0.106381588768\n",
      "Cost after 15000 iterations: 0.105720833553\n",
      "Cost after 16000 iterations: 0.105720833553\n",
      "Cost after 17000 iterations: 0.105060078337\n",
      "Cost after 18000 iterations: 0.105060078337\n",
      "Cost after 19000 iterations: 0.105720833553\n",
      "Cost after 20000 iterations: 0.105720833553\n",
      "Cost after 21000 iterations: 0.105060078337\n",
      "Cost after 22000 iterations: 0.105060078337\n",
      "Cost after 23000 iterations: 0.105060078337\n",
      "Cost after 24000 iterations: 0.102417057473\n",
      "Cost after 25000 iterations: 0.102417057473\n",
      "Cost after 26000 iterations: 0.102417057473\n",
      "Cost after 27000 iterations: 0.101756302257\n",
      "Cost after 28000 iterations: 0.102417057473\n",
      "Cost after 29000 iterations: 0.102417057473\n",
      "Training set accuracy: 99.5058%\n",
      "Test set accuracy: 98.7757%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 15.112132543070\n",
      "Cost after 1000 iterations: 0.770440580802\n",
      "Cost after 2000 iterations: 0.175100131228\n",
      "Cost after 3000 iterations: 0.129508021327\n",
      "Cost after 4000 iterations: 0.124221979599\n",
      "Cost after 5000 iterations: 0.122239713951\n",
      "Cost after 6000 iterations: 0.118275182656\n",
      "Cost after 7000 iterations: 0.116292917008\n",
      "Cost after 8000 iterations: 0.114310651360\n",
      "Cost after 9000 iterations: 0.114310651360\n",
      "Cost after 10000 iterations: 0.113649896144\n",
      "Cost after 11000 iterations: 0.113649896144\n",
      "Cost after 12000 iterations: 0.111667630496\n",
      "Cost after 13000 iterations: 0.113649896144\n",
      "Cost after 14000 iterations: 0.114310651360\n",
      "Cost after 15000 iterations: 0.111667630496\n",
      "Cost after 16000 iterations: 0.111006875280\n",
      "Cost after 17000 iterations: 0.111667630496\n",
      "Cost after 18000 iterations: 0.111006875280\n",
      "Cost after 19000 iterations: 0.111006875280\n",
      "Cost after 20000 iterations: 0.111667630496\n",
      "Cost after 21000 iterations: 0.111006875280\n",
      "Cost after 22000 iterations: 0.111667630496\n",
      "Cost after 23000 iterations: 0.111006875280\n",
      "Cost after 24000 iterations: 0.111667630496\n",
      "Cost after 25000 iterations: 0.111006875280\n",
      "Cost after 26000 iterations: 0.111667630496\n",
      "Cost after 27000 iterations: 0.111006875280\n",
      "Cost after 28000 iterations: 0.112328385712\n",
      "Cost after 29000 iterations: 0.112989140928\n",
      "Training set accuracy: 99.4548%\n",
      "Test set accuracy: 98.6481%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 11.098705361369\n",
      "Cost after 1000 iterations: 0.654147662794\n",
      "Cost after 2000 iterations: 0.175100131228\n",
      "Cost after 3000 iterations: 0.136776328702\n",
      "Cost after 4000 iterations: 0.134133307839\n",
      "Cost after 5000 iterations: 0.129508021327\n",
      "Cost after 6000 iterations: 0.129508021327\n",
      "Cost after 7000 iterations: 0.128847266111\n",
      "Cost after 8000 iterations: 0.129508021327\n",
      "Cost after 9000 iterations: 0.128186510895\n",
      "Cost after 10000 iterations: 0.127525755679\n",
      "Cost after 11000 iterations: 0.127525755679\n",
      "Cost after 12000 iterations: 0.127525755679\n",
      "Cost after 13000 iterations: 0.126204245247\n",
      "Cost after 14000 iterations: 0.126204245247\n",
      "Cost after 15000 iterations: 0.125543490031\n",
      "Cost after 16000 iterations: 0.124882734815\n",
      "Cost after 17000 iterations: 0.125543490031\n",
      "Cost after 18000 iterations: 0.125543490031\n",
      "Cost after 19000 iterations: 0.124882734815\n",
      "Cost after 20000 iterations: 0.125543490031\n",
      "Cost after 21000 iterations: 0.124882734815\n",
      "Cost after 22000 iterations: 0.126204245247\n",
      "Cost after 23000 iterations: 0.124882734815\n",
      "Cost after 24000 iterations: 0.125543490031\n",
      "Cost after 25000 iterations: 0.125543490031\n",
      "Cost after 26000 iterations: 0.125543490031\n",
      "Cost after 27000 iterations: 0.125543490031\n",
      "Cost after 28000 iterations: 0.124221979599\n",
      "Cost after 29000 iterations: 0.123561224383\n",
      "Training set accuracy: 99.4038%\n",
      "Test set accuracy: 98.6609%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 63, 6, 1], learning_rate=0.1, iterations=30000, regularization='l2', lambd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to simple linear regression on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 89.0285%\n",
      "Test set accuracy: 89.1468%\n"
     ]
    }
   ],
   "source": [
    "params = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, n=[126, 1], learning_rate=0.1, iterations=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, simple linear regression sees no performance gains from additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
