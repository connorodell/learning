{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Create a network with one hidden layer using vectorization as possible**\n",
    "\n",
    "The setup for this network is naive. It uses sigmoid activation functions in the hidden layer. Improvements could likely be acheived by replacing the sigmoid with tanh or ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, layer_size=2):\n",
    "    '''Returns initial parameters for a neural network with the given number of nodes in the hidden layer.\n",
    "    \n",
    "        X = n_x by m array of training examples\n",
    "        layer_size = number of nodes in hidden layer\n",
    "        \n",
    "    Returns four objects:\n",
    "    \n",
    "        w1 = n_x by layer_size array\n",
    "        w2 = layer_size by 1 array\n",
    "        b1 = 1 by layer_size array\n",
    "        b2 = float'''\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    w1 = np.random.randn(n_x, layer_size)\n",
    "    w2 = np.random.randn(layer_size, 1)\n",
    "    b1 = np.random.randn(1, layer_size)\n",
    "    b2 = np.random.randn()\n",
    "    \n",
    "    return w1, w2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Sigmoid/logistic function.\n",
    "    \n",
    "        x = numpy array or float'''\n",
    "    \n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y, yhat):\n",
    "    '''Loss function for logistic regression.\n",
    "    \n",
    "        y = true (binary) class\n",
    "        yhat = predicted class probability'''\n",
    "    \n",
    "    return -(y*np.log(yhat)+(1-y)*np.log(1-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(X, Y, w1, w2, b1, b2, layer_size=2):\n",
    "    '''Implement forward and backward propogation for simple logistic regression.\n",
    "    \n",
    "        X = n_x by m numpy array containg training data where n_x is the number of features and m is the number of samples\n",
    "        Y = 1 by m numpy array containing true sample classes\n",
    "        w = numpy array of model parameters\n",
    "        b = model bias array'''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    one_vec = np.full((m,1), 1)\n",
    "    \n",
    "    # forward propogation\n",
    "    # hidden layer\n",
    "    A1 = sigmoid(np.dot(X.T, w1)+b1) # m by layer_size array\n",
    "    \n",
    "    # output layer\n",
    "    A2 = sigmoid(np.dot(A1, w2)+b2) # m by 1 array\n",
    "    #print('A1 ', A1.shape)\n",
    "    #print('w2 ', w2.shape)\n",
    "    #print(A2)\n",
    "    \n",
    "    # backward propogation\n",
    "    dz1 = (1/m)*np.dot(w2, A2.T-Y)*(A1.T*(1-A1.T))\n",
    "    dw1 = np.dot(dz1, X.T)\n",
    "    dw2 = (1/m)*np.dot(A2.T-Y, A1)\n",
    "    db1 = np.dot(dz1, one_vec)\n",
    "    #db1 = (1/m)*np.dot(sum_vec, (A2-Y.T)*A1)\n",
    "    db2 = (1/m)*np.sum(A2-Y)\n",
    "    #print('db2 ', db2)\n",
    "    \n",
    "    gradient = dict({'dw1': dw1, 'dw2': dw2, 'db1': db1, 'db2': db2})\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, w1, w2, b1, b2, layer_size=2, iterations=2000, learning_rate=0.01):\n",
    "    '''Implement gradient descent fitting procedure.\n",
    "    \n",
    "        X = n_x by m array with samples in columns\n",
    "        Y = m by 1 array of true binary class values\n",
    "        w = array of model parameters\n",
    "        b = model bias array'''\n",
    "    \n",
    "    n_x = X.shape[0]    # number of features\n",
    "    m = X.shape[1]    # number of training samples\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        gradient = propogate(X, Y, w1, w2, b1, b2, layer_size=layer_size)\n",
    "        dw1 = gradient['dw1']\n",
    "        dw2 = gradient['dw2']\n",
    "        db1 = gradient['db1']\n",
    "        db2 = gradient['db2']\n",
    "        \n",
    "        w1 = w1 - learning_rate*dw1.T\n",
    "        w2 = w2 - learning_rate*dw2.T\n",
    "        b1 = b1 - learning_rate*db1.T\n",
    "        b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = dict({'w1': w1, 'w2': w2, 'b1': b1, 'b2': b2})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_val(x):\n",
    "    '''Find the most likely class given the input probability x.'''\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# use numpy vectorization to apply class_val to an array\n",
    "vclass_val = np.vectorize(class_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w1, w2, b1, b2):\n",
    "    '''Predict class using logistic regression model.\n",
    "    \n",
    "        X = array with n_x rows, samples in columns'''\n",
    "    \n",
    "    # hidden layer\n",
    "    A1 = sigmoid(np.dot(X.T, w1)+b1)\n",
    "    #print(w2)\n",
    "    \n",
    "    # output layer\n",
    "    out = sigmoid(np.dot(A1, w2)+b2)\n",
    "    \n",
    "    return vclass_val(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogModel(X_train, Y_train, X_test, Y_test, layer_size=2, iterations=2000, learning_rate=0.01):\n",
    "    '''Build logistic regression model using training data and test on provided\n",
    "    testing data.\n",
    "    \n",
    "        X_train = n_x by m array with feature vectors in columns\n",
    "        Y_train = m by 1 array of true binary class for each training sample\n",
    "        X_test = n_x by any number array with feature vectors in columns\n",
    "        Y_test = column vector of true binary class for each test sample\n",
    "        iterations = positive integer, number of iterations for gradient descent\n",
    "        learning_rate = positive float, learning rate used for gradient descent\n",
    "        \n",
    "    Prints percentage of correct predictions on testing data using the model fit by gradient descent.\n",
    "    Returns model parameters.'''\n",
    "    \n",
    "    w1, w2, b1, b2 = initialize(X_train, layer_size=layer_size)\n",
    "    #print(w1)\n",
    "    \n",
    "    parameters = fit(X_train, Y_train, w1, w2, b1, b2, layer_size=layer_size, iterations=iterations, learning_rate=learning_rate)\n",
    "    w1 = parameters['w1']\n",
    "    w2 = parameters['w2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    predictions = predict(X_test, w1, w2, b1, b2)\n",
    "    \n",
    "    #print(predictions)\n",
    "    \n",
    "    model_accuracy = 100-np.average(np.abs(Y_test-predictions.T))*100\n",
    "    \n",
    "    print('Model accuracy: {:.4f}%'.format(model_accuracy))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an OR neural network\n",
    "\n",
    "We overfit a neural network on a small problem. There are only four possible inputs for an OR statement, so we create a hidden node for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,1,0,0],[1,0,1,0]])\n",
    "Y = np.array([[1,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b1, b2 = initialize(X, layer_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw1': array([[ 0.00488399,  0.00421978],\n",
       "        [-0.02993307, -0.02312541],\n",
       "        [ 0.02339517,  0.01780057],\n",
       "        [-0.02207128, -0.02360303]]),\n",
       " 'dw2': array([[-0.13448924, -0.01580992, -0.14273217,  0.01480164]]),\n",
       " 'db1': array([[ 0.00127042],\n",
       "        [-0.01063261],\n",
       "        [ 0.00202261],\n",
       "        [ 0.00616183]]),\n",
       " 'db2': -0.5559064734264723}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propogate(X, Y, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 0.08480012,  0.75913017, -0.09740079, -0.59373514],\n",
       "        [ 0.87118126, -0.79815698,  1.09493664, -0.77825849]]),\n",
       " 'w2': array([[ 0.2016502 ],\n",
       "        [ 0.65945088],\n",
       "        [-0.36467286],\n",
       "        [ 0.69482114]]),\n",
       " 'b1': array([[ 1.11449646, -0.9474521 ,  1.01380003, -1.25132173]]),\n",
       " 'b2': 0.9738918458890288}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(X, Y, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = fit(X, Y, w1, w2, b1, b2, learning_rate=0.1)\n",
    "\n",
    "w1 = parameters['w1']\n",
    "w2 = parameters['w2']\n",
    "b1 = parameters['b1']\n",
    "b2 = parameters['b2']\n",
    "\n",
    "predict(X, w1, w2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[-3.36671083,  0.45851469, -1.83745462,  2.48526772],\n",
       "        [-3.46525019,  0.28770223, -0.69109538,  2.55158644]]),\n",
       " 'w2': array([[-6.08728712],\n",
       "        [-0.01699202],\n",
       "        [-1.76107814],\n",
       "        [ 4.58171016]]),\n",
       " 'b1': array([[ 1.56480151, -2.20350072, -0.22664747, -1.02659914]]),\n",
       " 'b2': 1.4013824428380999}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogModel(X, Y, X, Y, layer_size=4, learning_rate=0.1, iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the large dataset, the neural network performs well on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an AND network with different numbers of hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,1,0,0],[1,0,1,0]])\n",
    "Y = np.array([[1,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 1.98194352, -0.12378465,  0.50326089, -2.6868536 ],\n",
       "        [ 1.00824945, -1.9558207 ,  0.46871953, -2.70933873]]),\n",
       " 'w2': array([[ 3.41530518],\n",
       "        [-2.02150548],\n",
       "        [ 0.80051604],\n",
       "        [-6.22452884]]),\n",
       " 'b1': array([[-1.47613161,  0.38762502,  0.0556036 ,  3.68536558]]),\n",
       " 'b2': 0.06367765912859673}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogModel(X, Y, X, Y, layer_size=4, learning_rate=0.1, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 4.23705331, -1.08432874, -1.54101761],\n",
       "        [-0.27528058, -2.08697021, -3.15791841]]),\n",
       " 'w2': array([[ 4.16664998],\n",
       "        [-3.2545004 ],\n",
       "        [-5.09427344]]),\n",
       " 'b1': array([[-1.13777686,  1.68157315,  2.93043352]]),\n",
       " 'b2': -0.4855775069815068}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogModel(X, Y, X, Y, layer_size=3, learning_rate=0.1, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 3.08380884,  2.71167838],\n",
       "        [ 3.47127936, -0.84037907]]),\n",
       " 'w2': array([[7.71495827],\n",
       "        [1.2629936 ]]),\n",
       " 'b1': array([[-4.64059546,  0.01523817]]),\n",
       " 'b2': -5.467249181643752}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogModel(X, Y, X, Y, layer_size=2, learning_rate=0.1, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[-3.09408517],\n",
       "        [-3.0967422 ]]),\n",
       " 'w2': array([[-7.14181602]]),\n",
       " 'b1': array([[4.2203554]]),\n",
       " 'b2': 2.648310522930351}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogModel(X, Y, X, Y, layer_size=1, learning_rate=0.1, iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with 1 hidden layer does well on this problem for any number of hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NN on a large dataset\n",
    "\n",
    "Note: The data in this problem comes from an interview problem where the meaning of the variables and the output is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = '/Users/connorodell/Documents/Data_Science/learning/exercise_03_train.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data. Details can be found in other notebook\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# Correct days so that all are spelled out\n",
    "train_df['x35'] = train_df['x35'].map(lambda x: 'wednesday' if x=='wed' else 'thursday' if (x=='thur' or x=='thurday') else 'friday' if x=='fri' else x)\n",
    "\n",
    "# Correct sept. to Sept and Dev to Dec in column x68\n",
    "train_df['x68'] = train_df['x68'].map(lambda x: 'Jan' if x=='January' else 'Sept' if x=='sept.' else 'Dec' if x=='Dev' else x)\n",
    "\n",
    "# Transform columns x34, x35, x68, and x93 to dummy variables\n",
    "train_df = pd.get_dummies(train_df, columns=['x34', 'x35', 'x68', 'x93'])\n",
    "\n",
    "# Transform columns x41 and x45 to floats\n",
    "train_df['x41'] = train_df['x41'].map(lambda x: x.lstrip('$'))\n",
    "train_df['x41'] = pd.to_numeric(train_df['x41'])\n",
    "\n",
    "train_df['x45'] = train_df['x45'].map(lambda x: x.rstrip('%'))\n",
    "train_df['x45'] = pd.to_numeric(train_df['x45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('y', axis=1), train_df['y'], test_size=0.2, random_state=42)\n",
    "\n",
    "# scale data using the standard scaler in sklearn\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Get numpy columns for Y\n",
    "Y_train = Y_train.values.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.values.reshape(len(Y_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "# run new model\n",
    "# note that samples are in rows of train_df, so the transposes are fed into the models\n",
    "params = LogModel(X_train.T, Y_train.T, X_test.T, Y_test.T, layer_size=3, learning_rate=0.1, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* The current neural network framework with the sigmoid activation functions does not perform well on the large train_df dataset. It may improve with more iterations, but the time required for each iteration with the current framework is large.\n",
    "* It will be interesting to see how improved versions of a neural network with tanh or ReLU activation functions perform.\n",
    "* The amount of time required to train the network is large, probably due to multiplication of large matrices.\n",
    "* We get an overflow warning from the np.exp function when the number of nodes in the hidden layer is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 79.6837%\n"
     ]
    }
   ],
   "source": [
    "params = LogModel(X_train.T, Y_train.T, X_test.T, Y_test.T, layer_size=60, learning_rate=0.1, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Increasing the number of nodes in the hidden layer doesn't seem to have any effect on accuracy but greatly increases the time required to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
